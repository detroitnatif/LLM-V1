{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a0b84cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "08b68e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "i2s = {p+1:l for p, l in enumerate(chars)}\n",
    "i2s[0] = '.'\n",
    "\n",
    "s2i = {}\n",
    "\n",
    "for i, l in i2s.items():\n",
    "    s2i[l] = i\n",
    "    \n",
    "vocab_size = len(s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "75ac032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch] \n",
    "            Y.append(ix)\n",
    "            X.append(context)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(.8*len(words))\n",
    "n2 = int(.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train_words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # dev_words\n",
    "Xte, Yte = build_dataset(words[n2:]) # test_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "11c15cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 45597 parameters in total\n"
     ]
    }
   ],
   "source": [
    "# SAME INSTANCES OF PYTORCH API BUT BUILT OUT WITH LESS FUNCTIONALITY TO UNDERSTAND\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# C = torch.randn((vocab_size, n_embd),            generator=g)  REMOVED THIS WITH CLASS: EMBEDDING\n",
    "        \n",
    "              # THIS WAS THE PREVIOUS WAY TO DO THIS, Sequential class fixes this\n",
    "# layers = [  # THIS IS CREATING A LIST WHERE EACH TRANSFORM WILL HAPPEN::: EMB => Flat => WEIGHTS => BATCHNORM => TanH\n",
    "#   Embedding(vocab_size, n_embd), Flatten(),\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "# ]\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "#   layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0 #5/3\n",
    "\n",
    "         \n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "params = sum(params.nelement() for params in parameters)\n",
    "print(f\"there are {params} parameters in total\") # number of parameters in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "680fa099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/    200: 1.9719\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "      \n",
    "  # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # CREATES RAND INTS OF \"BATCH SIZE\" FROM 0 => Training size\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  #   forward pass          SIMPLIFIED BY ADDING EMBED AND FLATTEN CLASSES\n",
    "#     emb = C[Xb] # embed the characters into vectors\n",
    "#     print(emb.shape)\n",
    "#     x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "#     x = Xb\n",
    "#     for layer in layers:  # LOOP THE LAYERS AND PERFORM TRANSFORM\n",
    "        \n",
    "\n",
    "    logits = model(Xb) # CALLING THE SEQUENTIAL CLASS MADE EARLIER\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    \n",
    "\n",
    "  # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "  # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    if i >= 1000:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"training network ...\")\n",
    "#         print(\"Test Stage - Remove Break\")\n",
    "#         break # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "901ff654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5d681d6d0>]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "lossi_tensor = torch.tensor(lossi)\n",
    "plt.plot(lossi_tensor.view(-1, 200).mean(1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d8242019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [213]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m   ix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;28mprint\u001b[39m(ix)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# shift the context window and track the samples\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 8\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "#         emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "#         x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        \n",
    "        logits = model(torch.tensor([context]))\n",
    "        print(logits)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        print(ix)\n",
    "      # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(i2s[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dee92f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random index to create batch\n",
      "tensor([  8384, 131662,  18388,    139])\n",
      "----------------- \n",
      "\n",
      "Xtr is just a collection of tensors which are the indexs of each letter\n",
      "Entry 4 into Xtr tensor([ 0,  0,  0,  0, 25, 21,  8,  5])\n",
      "----------------- \n",
      "\n",
      "Index into batch\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  8, 15],\n",
      "        [ 0,  0,  0,  0, 10, 15, 18,  1],\n",
      "        [ 0,  0,  0,  0,  0,  3, 15, 12]])\n",
      "----------------- \n",
      "\n",
      "Model is being called on the batch, giving probs of the next coming layer\n",
      "tensor([-0.9500,  3.0625, -0.9499,  1.1203,  0.0969,  1.9696, -0.1247, -0.7200,\n",
      "         0.8801,  1.1345,  0.7342,  1.3787, -0.0368,  0.7505, -0.4440,  0.3457,\n",
      "        -1.0537, -1.7964,  0.8402,  0.4152,  0.7981, -1.1519, -1.2541, -1.1436,\n",
      "        -1.5343,  0.3755, -0.3279], grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "Call softmax function to normalize the probs of next coming layer\n",
      "tensor([0.0063, 0.3479, 0.0063, 0.0499, 0.0179, 0.1167, 0.0144, 0.0079, 0.0392,\n",
      "        0.0506, 0.0339, 0.0646, 0.0157, 0.0345, 0.0104, 0.0230, 0.0057, 0.0027,\n",
      "        0.0377, 0.0247, 0.0361, 0.0051, 0.0046, 0.0052, 0.0035, 0.0237, 0.0117],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "I am confused how this index is added to create the next letter\n",
      "tensor([[ 1],\n",
      "        [ 4],\n",
      "        [13],\n",
      "        [ 9]])\n",
      "----------------- \n",
      "\n",
      "This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \n",
      "(originally 10 in example)\n",
      "torch.Size([4, 8, 10])\n",
      "----------------- \n",
      "\n",
      "This is the flatten layer, which takes the 8 context and 10 dimensions and but seperates them into \n",
      "4 groups of 2 by 10 dimensions\n",
      "torch.Size([4, 4, 20])\n",
      "----------------- \n",
      "\n",
      "This is the linear layer, which takes the flattened matrix and multiplies by n amount of neurons\n",
      "torch.Size([4, 4, 100])\n",
      "----------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALISATION OF MODEL\n",
    "\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,)) # GETTING 4 examples of random integers ie => tensor([37505, 99034,  6899, 19835])\n",
    "print(\"random index to create batch\")\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Xtr is just a collection of tensors which are the indexs of each letter\")\n",
    "print(f\"Entry 4 into Xtr {Xtr[4]}\")\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Index into batch\")\n",
    "Xb = Xtr[ix]                              # Batch by indexing into training data by 4 randoms places \n",
    "print(Xb)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Model is being called on the batch, giving probs of the next coming layer\")\n",
    "logits = model(Xb)\n",
    "print(logits[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Call softmax function to normalize the probs of next coming layer\")\n",
    "probs=F.softmax(logits, dim=1)\n",
    "print(probs[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"I am confused how this index is added to create the next letter\")\n",
    "ix = torch.multinomial(probs, num_samples=1, generator=g)\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \\n(originally 10 in example)\")\n",
    "print(model.layers[0].out.shape)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"This is the flatten layer, which takes the 8 context and 10 dimensions and but seperates them into \\n4 groups of 2 by 10 dimensions\")\n",
    "print(model.layers[1].out.shape)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"This is the linear layer, which takes the flattened matrix and multiplies by n amount of neurons\")\n",
    "print(model.layers[2].out.shape)\n",
    "print(\"----------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8e09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
