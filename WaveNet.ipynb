{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0b592c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ea248425",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "i2s = {p+1:l for p, l in enumerate(chars)}\n",
    "i2s[0] = '.'\n",
    "\n",
    "s2i = {}\n",
    "\n",
    "for i, l in i2s.items():\n",
    "    s2i[l] = i\n",
    "    \n",
    "vocab_size = len(s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "544347f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch] \n",
    "            Y.append(ix)\n",
    "            X.append(context)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(.8*len(words))\n",
    "n2 = int(.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train_words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # dev_words\n",
    "Xte, Yte = build_dataset(words[n2:]) # test_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "12fe2781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 45597 parameters in total\n"
     ]
    }
   ],
   "source": [
    "# SAME INSTANCES OF PYTORCH API BUT BUILT OUT WITH LESS FUNCTIONALITY TO UNDERSTAND\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# C = torch.randn((vocab_size, n_embd),            generator=g)  REMOVED THIS WITH CLASS: EMBEDDING\n",
    "        \n",
    "              # THIS WAS THE PREVIOUS WAY TO DO THIS, Sequential class fixes this\n",
    "# layers = [  # THIS IS CREATING A LIST WHERE EACH TRANSFORM WILL HAPPEN::: EMB => Flat => WEIGHTS => BATCHNORM => TanH\n",
    "#   Embedding(vocab_size, n_embd), Flatten(),\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "# ]\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "#   layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0 #5/3\n",
    "\n",
    "         \n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "params = sum(params.nelement() for params in parameters)\n",
    "print(f\"there are {params} parameters in total\") # number of parameters in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3e7c76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/    200: 1.9719\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "      \n",
    "  # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # CREATES RAND INTS OF \"BATCH SIZE\" FROM 0 => Training size\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  #   forward pass          SIMPLIFIED BY ADDING EMBED AND FLATTEN CLASSES\n",
    "#     emb = C[Xb] # embed the characters into vectors\n",
    "#     print(emb.shape)\n",
    "#     x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "#     x = Xb\n",
    "#     for layer in layers:  # LOOP THE LAYERS AND PERFORM TRANSFORM\n",
    "        \n",
    "\n",
    "    logits = model(Xb) # CALLING THE SEQUENTIAL CLASS MADE EARLIER\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    \n",
    "\n",
    "  # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "  # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    if i >= 1000:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"training network ...\")\n",
    "#         print(\"Test Stage - Remove Break\")\n",
    "#         break # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4eb3d314",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [199]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     layer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m lossi_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lossi)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "for layer in model.layers():\n",
    "    layer.training = False\n",
    "\n",
    "lossi_tensor = torch.tensor(lossi)\n",
    "plt.plot(lossi_tensor.view(-1, 200).mean(1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 8\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "#         emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "#         x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        \n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        print(ix)\n",
    "      # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(i2s[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f0ea5a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random index to create batch\n",
      "tensor([ 2126,  8981, 76622, 22715])\n",
      "----------------- \n",
      "\n",
      "Xtr is just a collection of tensors which are the indexs of each letter\n",
      "Entry 4 into Xtr tensor([ 0,  0,  0,  0, 25, 21,  8,  5])\n",
      "----------------- \n",
      "\n",
      "Index into batch\n",
      "tensor([[ 0,  0,  0,  0,  0,  0, 10,  1],\n",
      "        [ 0,  0,  0,  0, 20, 18,  9, 14],\n",
      "        [ 0,  0,  0,  0,  0,  0,  1, 12],\n",
      "        [ 0,  0,  0,  0,  0, 19,  1, 14]])\n",
      "----------------- \n",
      "\n",
      "Model is being called on the batch, giving probs of the next coming layer\n",
      "tensor([-1.1875,  1.1927, -0.1675,  0.6924,  0.0183,  0.1868, -1.1468, -1.7299,\n",
      "         2.0555, -0.5519, -0.9852,  1.6306,  0.7265,  1.5741,  2.3115, -0.5460,\n",
      "        -1.1175, -2.3534,  2.7257,  0.3001, -0.4683, -0.2539,  0.1758, -1.5664,\n",
      "        -1.7866,  0.7639,  0.4203], grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "Call softmax function to normalize the probs of next coming layer\n",
      "tensor([0.0047, 0.0513, 0.0132, 0.0311, 0.0158, 0.0188, 0.0049, 0.0028, 0.1215,\n",
      "        0.0090, 0.0058, 0.0795, 0.0322, 0.0751, 0.1570, 0.0090, 0.0051, 0.0015,\n",
      "        0.2375, 0.0210, 0.0097, 0.0121, 0.0185, 0.0032, 0.0026, 0.0334, 0.0237],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "I am confused how this index is added to create the next letter\n",
      "tensor([[8],\n",
      "        [0],\n",
      "        [9],\n",
      "        [0]])\n",
      "----------------- \n",
      "\n",
      "This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \n",
      "(originally 10 in example)\n",
      "torch.Size([4, 8, 10])\n",
      "----------------- \n",
      "\n",
      "This is the flatten layer, which takes the 8 context and 10 dimensions and flattens them\n",
      "torch.Size([4, 4, 20])\n",
      "----------------- \n",
      "\n",
      "This is the linear layer, which takes the flattened matrix and multiplies in into n amount of neurons\n",
      "torch.Size([4, 4, 100])\n",
      "----------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALISATION OF MODEL\n",
    "\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,)) # GETTING 4 examples of random integers ie => tensor([37505, 99034,  6899, 19835])\n",
    "print(\"random index to create batch\")\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Xtr is just a collection of tensors which are the indexs of each letter\")\n",
    "print(f\"Entry 4 into Xtr {Xtr[4]}\")\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Index into batch\")\n",
    "Xb = Xtr[ix]                              # Batch by indexing into training data by 4 randoms places \n",
    "print(Xb)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Model is being called on the batch, giving probs of the next coming layer\")\n",
    "logits = model(Xb)\n",
    "print(logits[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Call softmax function to normalize the probs of next coming layer\")\n",
    "probs=F.softmax(logits, dim=1)\n",
    "print(probs[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"I am confused how this index is added to create the next letter\")\n",
    "ix = torch.multinomial(probs, num_samples=1, generator=g)\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \\n(originally 10 in example)\")\n",
    "print(model.layers[0].out.shape)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"This is the flatten layer, which takes the 8 context and 10 dimensions and flattens them\")\n",
    "print(model.layers[1].out.shape)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"This is the linear layer, which takes the flattened matrix and multiplies by n amount of neurons\")\n",
    "print(model.layers[2].out.shape)\n",
    "print(\"----------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558d060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
