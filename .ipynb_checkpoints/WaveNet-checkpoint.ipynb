{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "472f7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05fba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "i2s = {p+1:l for p, l in enumerate(chars)}\n",
    "i2s[0] = '.'\n",
    "\n",
    "s2i = {}\n",
    "\n",
    "for i, l in i2s.items():\n",
    "    s2i[l] = i\n",
    "    \n",
    "vocab_size = len(s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "30d73db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s2i[ch] \n",
    "            Y.append(ix)\n",
    "            X.append(context)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(.8*len(words))\n",
    "n2 = int(.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # train_words\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # dev_words\n",
    "Xte, Yte = build_dataset(words[n2:]) # test_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff5f3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 76579 parameters in total\n"
     ]
    }
   ],
   "source": [
    "# SAME INSTANCES OF PYTORCH API BUT BUILT OUT WITH LESS FUNCTIONALITY TO UNDERSTAND\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# C = torch.randn((vocab_size, n_embd),            generator=g)  REMOVED THIS WITH CLASS: EMBEDDING\n",
    "        \n",
    "              # THIS WAS THE PREVIOUS WAY TO DO THIS, Sequential class fixes this\n",
    "# layers = [  # THIS IS CREATING A LIST WHERE EACH TRANSFORM WILL HAPPEN::: EMB => Flat => WEIGHTS => BATCHNORM => TanH\n",
    "#   Embedding(vocab_size, n_embd), Flatten(),\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "# ]\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "    layers[-1].gamma *= 0.1\n",
    "#   layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 1.0 #5/3\n",
    "\n",
    "         \n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "params = sum(params.nelement() for params in parameters)\n",
    "print(f\"there are {params} parameters in total\") # number of parameters in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2579d691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/   5000: 3.5462\n",
      "training network ...\n",
      "training network ...\n",
      "training network ...\n",
      "training network ...\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 5000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "      \n",
    "  # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g) # CREATES RAND INTS OF \"BATCH SIZE\" FROM 0 => Training size\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  #   forward pass          SIMPLIFIED BY ADDING EMBED AND FLATTEN CLASSES\n",
    "#     emb = C[Xb] # embed the characters into vectors\n",
    "#     print(emb.shape)\n",
    "#     x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "#     x = Xb\n",
    "#     for layer in layers:  # LOOP THE LAYERS AND PERFORM TRANSFORM\n",
    "        \n",
    "\n",
    "    logits = model(Xb) # CALLING THE SEQUENTIAL CLASS MADE EARLIER\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    \n",
    "\n",
    "  # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "  \n",
    "  # update\n",
    "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    if i >= 1000:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"training network ...\")\n",
    "#         print(\"Test Stage - Remove Break\")\n",
    "#         break # AFTER_DEBUG: would take out obviously to run full optimization\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a0905b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [165]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     layer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m lossi_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lossi)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "for layer in model.layers():\n",
    "    layer.training = False\n",
    "\n",
    "lossi_tensor = torch.tensor(lossi)\n",
    "plt.plot(lossi_tensor.view(-1, 200).mean(1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28ef8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/_7z9t82101973t_1gxmf2vwc0000gn/T/ipykernel_9104/2383813497.py:40: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
      "  xvar = x.var(dim, keepdim=True) # batch variance\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m   ix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28mprint\u001b[39m(ix)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# shift the context window and track the samples\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 8\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "#         emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "#         x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        \n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        print(ix)\n",
    "      # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(i2s[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "39290d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get index from random by batch size\n",
      "tensor([ 97392,  76131, 162632,  77689])\n",
      "----------------- \n",
      "\n",
      "Xtr is just a collection of tensors which are the indexs of each letter\n",
      "Entry 4 into Xtr tensor([ 0,  0,  0,  0, 20,  1, 21, 18])\n",
      "----------------- \n",
      "\n",
      "Index into batch\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0, 13],\n",
      "        [ 0,  5, 19, 20,  5, 16,  8,  1],\n",
      "        [ 0,  0, 25, 15, 18, 12,  5, 14],\n",
      "        [ 0,  0,  0,  0,  0,  0, 26,  1]])\n",
      "----------------- \n",
      "\n",
      "Model is being called on the batch, giving probs of the next coming layer\n",
      "tensor([-0.2693,  0.9588, -0.5150, -0.6870, -0.7513,  0.0773,  1.1154,  1.2590,\n",
      "         0.0677, -0.6649,  0.3882, -0.4877,  0.1590, -0.6596, -0.3645,  0.9785,\n",
      "        -0.8471,  0.2722, -0.7130, -0.8645, -0.3863, -0.6536, -0.5855, -0.2974,\n",
      "         0.0298,  0.4642,  0.3173], grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "Call softmax function to normalize the probs of next coming layer\n",
      "tensor([0.0251, 0.0856, 0.0196, 0.0165, 0.0155, 0.0355, 0.1002, 0.1156, 0.0351,\n",
      "        0.0169, 0.0484, 0.0202, 0.0385, 0.0170, 0.0228, 0.0873, 0.0141, 0.0431,\n",
      "        0.0161, 0.0138, 0.0223, 0.0171, 0.0183, 0.0244, 0.0338, 0.0522, 0.0451],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "----------------- \n",
      "\n",
      "I am confused how this index is added to create the next letter\n",
      "tensor([[ 1],\n",
      "        [19],\n",
      "        [17],\n",
      "        [23]])\n",
      "----------------- \n",
      "\n",
      "This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \n",
      "(originally 10 in example)\n",
      "torch.Size([4, 8, 24])\n"
     ]
    }
   ],
   "source": [
    "# VISUALISATION OF MODEL\n",
    "\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,)) # GETTING 4 examples of random integers ie => tensor([37505, 99034,  6899, 19835])\n",
    "print(\"Get index from random by batch size\")\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Xtr is just a collection of tensors which are the indexs of each letter\")\n",
    "print(f\"Entry 4 into Xtr {Xtr[4]}\")\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Index into batch\")\n",
    "Xb = Xtr[ix]                              # Batch by indexing into training data by 4 randoms places \n",
    "print(Xb)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"Model is being called on the batch, giving probs of the next coming layer\")\n",
    "logits = model(Xb)\n",
    "print(logits[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"Call softmax function to normalize the probs of next coming layer\")\n",
    "probs=F.softmax(logits, dim=1)\n",
    "print(probs[0])\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"I am confused how this index is added to create the next letter\")\n",
    "ix = torch.multinomial(probs, num_samples=1, generator=g)\n",
    "print(ix)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "\n",
    "print(\"This is the embedding layer, which takes the 4 samples X 8 context and adds n dimensions \\n(originally 10 in example)\")\n",
    "print(model.layers[0].out.shape)\n",
    "print(\"----------------- \\n\")\n",
    "\n",
    "print(\"This is the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d4141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
